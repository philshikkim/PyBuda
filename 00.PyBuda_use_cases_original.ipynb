{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ca71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Â© 2024 Tenstorrent AI ULC\n",
    "\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "# Test \"user experience\" scenarios, i.e. different ways to use the API to run things on TT hardware\n",
    "# Each test intentionally creates everything from scratch and uses no verification env, so that each\n",
    "# of these tests can be used as user examples.\n",
    "# There's also no verification of correctness of data, as that's not the point of these tests.\n",
    "#\n",
    "# All of these tests will run on silicon, in concurrent mode, by default. However, setting \n",
    "# PYBUDA_DEVMODE=1 env variable will drop them into Golden+sequential mode.\n",
    "\n",
    "import queue\n",
    "import torch\n",
    "import pybuda\n",
    "import pytest\n",
    "from pybuda.config import _get_global_compiler_config\n",
    "\n",
    "from pybuda.schedulers import LearningRateScheduler\n",
    "from pybuda.pybudaglobal import pybuda_reset\n",
    "from pybuda._C.backend_api import BackendDevice, BackendType, DeviceMode \n",
    "from test.utils import download_model\n",
    "\n",
    "# https://github.com/pytorch/pytorch/wiki/Autograd-and-Fork\n",
    "mp_context = torch.multiprocessing.get_context('spawn')\n",
    "\n",
    "def _safe_read(q):\n",
    "    \"\"\"\n",
    "    Read a queue, but return None if an error was raised in the meantime, preventing a hang on error.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            data = q.get(timeout = 0.5)\n",
    "            return data\n",
    "        except queue.Empty as _:\n",
    "            if pybuda.error_raised():\n",
    "                raise RuntimeError(\"Error raised in pybuda\")\n",
    "        except KeyboardInterrupt:\n",
    "            return None\n",
    "\n",
    "# Sample PyBuda module\n",
    "class PyBudaTestModule(pybuda.PyBudaModule):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "        self.weights1 = pybuda.Parameter(torch.rand(32, 32), requires_grad=True)\n",
    "        self.weights2 = pybuda.Parameter(torch.rand(32, 32), requires_grad=True)\n",
    "\n",
    "    def forward(self, act1, act2):\n",
    "        m1 = pybuda.op.Matmul(\"matmul1\", act1, self.weights1)\n",
    "        m2 = pybuda.op.Matmul(\"matmul2\", act2, self.weights2)\n",
    "        return m1 + m2, m2\n",
    "\n",
    "# Sample PyBuda module\n",
    "class PyBudaTestModuleOneOut(pybuda.PyBudaModule):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "        self.weights1 = pybuda.Parameter(torch.rand(32, 32), requires_grad=True)\n",
    "        self.weights2 = pybuda.Parameter(torch.rand(32, 32), requires_grad=True)\n",
    "\n",
    "    def forward(self, act1, act2):\n",
    "        m1 = pybuda.op.Matmul(\"matmul1\", act1, self.weights1)\n",
    "        m2 = pybuda.op.Matmul(\"matmul2\", act2, self.weights2)\n",
    "        return m1 + m2\n",
    "\n",
    "# Sample PyBuda module\n",
    "class PyBudaTestQueryKeyModule(pybuda.PyBudaModule):\n",
    "    def __init__(self, name, hidden_dim = 128, num_heads = 4):\n",
    "        super().__init__(name)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.key_weights = pybuda.Parameter(torch.rand(1, 1, hidden_dim, hidden_dim), requires_grad=True)\n",
    "        self.query_weights = pybuda.Parameter(torch.rand(1, 1, hidden_dim, hidden_dim), requires_grad=True)\n",
    "        self.value_weights = pybuda.Parameter(torch.rand(1, 1, hidden_dim, hidden_dim), requires_grad=True)\n",
    "\n",
    "    def forward(self, encoder_input):\n",
    "        query = pybuda.op.Matmul(f\"mha_query\", encoder_input, self.query_weights)\n",
    "        query = pybuda.op.HSlice(f\"mha_query_slice\", query, self.num_heads)\n",
    "\n",
    "        key = pybuda.op.Matmul(f\"mha_key\", encoder_input, self.key_weights)\n",
    "        key = pybuda.op.HSlice(f\"mha_key_slice\", key, self.num_heads)\n",
    "        key = pybuda.op.Transpose(f\"mha_key_transpose\", key, 2, 3)\n",
    "\n",
    "        attention_scores = pybuda.op.Matmul(f\"mha_as\", query, key)\n",
    "        return attention_scores\n",
    "\n",
    "\n",
    "class PyBudaTestForkWithThreeUsers(pybuda.PyBudaModule):\n",
    "    def __init__(self, name, hidden_dim = 128, num_heads = 4):\n",
    "        super().__init__(name)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.mm_a_weights = pybuda.Parameter(torch.rand(1, 1, hidden_dim, hidden_dim), requires_grad=True)\n",
    "        self.mm_b_weights = pybuda.Parameter(torch.rand(1, 1, hidden_dim, hidden_dim), requires_grad=True)\n",
    "        self.mm_c_weights = pybuda.Parameter(torch.rand(1, 1, hidden_dim, hidden_dim), requires_grad=True)\n",
    "\n",
    "    def forward(self, encoder_input):\n",
    "        a = pybuda.op.Matmul(f\"mm_a\", encoder_input, self.mm_a_weights)\n",
    "        b = pybuda.op.Matmul(f\"mm_b\", encoder_input, self.mm_b_weights)\n",
    "        c = pybuda.op.Matmul(f\"mm_c\", encoder_input, self.mm_c_weights)\n",
    "\n",
    "        add_a_b = pybuda.op.Add(f\"add_a_b\", a, b)\n",
    "        add_a_b_c = pybuda.op.Add(f\"add_a_b_c\", add_a_b, c)\n",
    "        return add_a_b_c\n",
    "\n",
    "\n",
    "\n",
    "# Sample PyTorch module\n",
    "class PyTorchTestModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights1 = torch.nn.Parameter(torch.rand(32, 32), requires_grad=True)\n",
    "        self.weights2 = torch.nn.Parameter(torch.rand(32, 32), requires_grad=True)\n",
    "\n",
    "    def forward(self, act1, act2):\n",
    "        m1 = torch.matmul(act1, self.weights1)\n",
    "        m2 = torch.matmul(act2, self.weights2)\n",
    "        return m1 + m2, m1\n",
    "\n",
    "# Sample PyTorch module\n",
    "class PyTorchTestModuleOneOut(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights1 = torch.nn.Parameter(torch.rand(32, 32), requires_grad=True)\n",
    "        self.weights2 = torch.nn.Parameter(torch.rand(32, 32), requires_grad=True)\n",
    "\n",
    "    def forward(self, act1, act2):\n",
    "        m1 = torch.matmul(act1, self.weights1)\n",
    "        m2 = torch.matmul(act2, self.weights2)\n",
    "        return m1 + m2\n",
    "\n",
    "class PyTorchTestModuleOneInputAndOneOut(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.rand(32, 32), requires_grad=True)\n",
    "    \n",
    "    def forward(self, act):\n",
    "        m = torch.matmul(act, self.weights)\n",
    "        return m\n",
    "\n",
    "class PyTorchLoss(torch.nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.sum()\n",
    "\n",
    "#\n",
    "# Run inference on module directly\n",
    "#\n",
    "def test_module_direct_pybuda():\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "\n",
    "    # Run single inference pass on a PyBuda module directly\n",
    "    output = PyBudaTestModule(\"direct\").run(input1, input2)\n",
    "    print(output)\n",
    "\n",
    "def test_module_direct_pytorch():\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "\n",
    "    # Run single inference pass on a PyTorch module, using a wrapper to convert to PyBuda first\n",
    "    output = pybuda.PyTorchModule(\"direct_pt\", PyTorchTestModule()).run(input1, input2)\n",
    "    print(output)\n",
    "\n",
    "#\n",
    "# Run inference through run_inference without placing on device\n",
    "#\n",
    "def test_run_inference_direct_pybuda():\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "\n",
    "    # Run inference on a PyBuda module, with given inputs\n",
    "    inputs = {\"act2\" : input2, \"act1\" : input1}\n",
    "    output_q = pybuda.run_inference(PyBudaTestModule(\"run_direct\"), inputs=[inputs])\n",
    "    output = _safe_read(output_q)\n",
    "    print(output)\n",
    "\n",
    "def test_run_inference_direct_pytorch():\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "\n",
    "    # Run inference, using a wrapper to convert PyTorch module to PyBuda, and with given inputs\n",
    "    inputs = {\"act2\" : input2, \"act1\" : input1}\n",
    "    output_q = pybuda.run_inference(pybuda.PyTorchModule(\"run_direct_pt\", PyTorchTestModule()), inputs=[inputs])\n",
    "    output = _safe_read(output_q)\n",
    "    print(output)\n",
    "\n",
    "\n",
    "#\n",
    "# Run inference by placing on device first\n",
    "#\n",
    "def test_run_inference_placed_pybuda():\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "\n",
    "    # Create a TT device\n",
    "    tt0 = pybuda.TTDevice(\"tt0\")\n",
    "\n",
    "    # Place a module on the device\n",
    "    tt0.place_module(PyBudaTestModule(\"placed\"))\n",
    "\n",
    "    # Push intputs to the device\n",
    "    tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "    # Run pipeline, and read the outputs\n",
    "    output_q = pybuda.run_inference()\n",
    "    output = _safe_read(output_q)\n",
    "    print(output)\n",
    "\n",
    "def test_run_inference_placed_pytorch():\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "\n",
    "    # Create a TT device\n",
    "    tt0 = pybuda.TTDevice(\"tt0\")\n",
    "\n",
    "    # Place a module on the device, using a wrapper to convert PyTorch module to PyBuda\n",
    "    tt0.place_module(pybuda.PyTorchModule(\"placed_pt\", PyTorchTestModule()))\n",
    "    \n",
    "    # Push intputs to the device\n",
    "    tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "    # Run pipeline, and read the outputs\n",
    "    output_q = pybuda.run_inference()\n",
    "    output = _safe_read(output_q)\n",
    "    print(output)\n",
    "\n",
    "#\n",
    "# Repeated calls to run inference on the same module\n",
    "#\n",
    "def test_module_direct_repeated():\n",
    "    module = PyBudaTestModule(\"direct\")\n",
    "\n",
    "    # Run on given inputs\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "    output = module.run(input1, input2)\n",
    "    print(output)\n",
    "\n",
    "    # Run again, without recompiling\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "    output = module.run(input1, input2)\n",
    "    print(output)\n",
    "\n",
    "    # Run again, without recompiling\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "    output = module.run(input1, input2)\n",
    "    print(output)\n",
    "\n",
    "def test_run_inference_placed_repeated():\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "    tt0 = pybuda.TTDevice(\"tt0\")\n",
    "    tt0.place_module(PyBudaTestModule(\"placed\"))\n",
    "\n",
    "    # Push one input and run\n",
    "    tt0.push_to_inputs((input1, input2))\n",
    "    output_q = pybuda.run_inference()\n",
    "\n",
    "    output = _safe_read(output_q)\n",
    "    print(output)\n",
    "\n",
    "    # Push two more inputs, and run one more time on both inputs, without recompiling\n",
    "    for _ in range(2):\n",
    "        input1 = torch.rand(4, 32, 32)\n",
    "        input2 = torch.rand(4, 32, 32)\n",
    "        tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "    pybuda.run_inference(input_count=2)\n",
    "\n",
    "    for _ in range(2):\n",
    "        output = _safe_read(output_q)\n",
    "        print(output)\n",
    "\n",
    "\n",
    "#\n",
    "# Run inference through setup + run_forward calls\n",
    "#\n",
    "def test_setup_forward_calls():\n",
    "    tt0 = pybuda.TTDevice(\"tt0\")\n",
    "    tt0.place_module(PyBudaTestModule(\"placed\"))\n",
    "\n",
    "    # Compile & initialize the pipeline for inference, with given shapes\n",
    "    output_q = pybuda.initialize_pipeline(training=False, sample_inputs=(torch.rand(4, 32, 32), torch.rand(4, 32, 32)))\n",
    "        \n",
    "    # Push & run_forward manually\n",
    "    for _ in range(2):\n",
    "        input1 = torch.rand(4, 32, 32)\n",
    "        input2 = torch.rand(4, 32, 32)\n",
    "        tt0.push_to_inputs((input1, input2))\n",
    "        pybuda.run_forward(input_count=1)\n",
    "\n",
    "        print(_safe_read(output_q))\n",
    "\n",
    "\n",
    "#\n",
    "# Run inference in concurrent mode, then push more inputs afterwards (won't work on Golden)\n",
    "#\n",
    "def test_run_inference_delayed_push():\n",
    "    \n",
    "    #### Skip the test on golden\n",
    "    import os\n",
    "    if \"PYBUDA_DEVMODE\" in os.environ:\n",
    "        pytest.skip()\n",
    "    ####\n",
    "\n",
    "    tt0 = pybuda.TTDevice(\"tt0\")\n",
    "    tt0.place_module(PyBudaTestModule(\"placed\"))\n",
    "\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "    tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "    # Run with input count 3, although only one is pushed\n",
    "    output_q = pybuda.run_inference(input_count=3)\n",
    "\n",
    "    # Read one output that should've been produced\n",
    "    output = _safe_read(output_q)\n",
    "    print(output)\n",
    "\n",
    "    # The inference thread is running in the background, waiting for data. Let's push two more.\n",
    "    for _ in range(2):\n",
    "        input1 = torch.rand(4, 32, 32)\n",
    "        input2 = torch.rand(4, 32, 32)\n",
    "        tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "    # Read two more outputs\n",
    "    for _ in range(2):\n",
    "        output = _safe_read(output_q)\n",
    "        print(output)\n",
    "\n",
    "#\n",
    "# Run inference on multiple devices - combinations of cpu / tt device\n",
    "#\n",
    "def test_cpu_tt_pipeline():\n",
    "\n",
    "    cpu0 = pybuda.CPUDevice(\"cpu0\")\n",
    "    cpu0.place_module(pybuda.PyTorchModule(\"stage0\", PyTorchTestModule()))\n",
    "    tt1 = pybuda.TTDevice(\"tt1\")\n",
    "    tt1.place_module(PyBudaTestModule(\"stage1\"))\n",
    "\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "    cpu0.push_to_inputs((input1, input2))\n",
    "\n",
    "    output_q = pybuda.run_inference()\n",
    "    print(_safe_read(output_q))\n",
    "\n",
    "def test_cpu_tt_pipeline_compact():\n",
    "\n",
    "    cpu0 = pybuda.CPUDevice(\"cpu0\", module=pybuda.PyTorchModule(\"stage0\", PyTorchTestModule()))\n",
    "    tt1 = pybuda.TTDevice(\"tt1\", module=PyBudaTestModule(\"stage1\"))\n",
    "\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "    cpu0.push_to_inputs((input1, input2))\n",
    "\n",
    "    output_q = pybuda.run_inference()\n",
    "    print(_safe_read(output_q))\n",
    "\n",
    "# Run training, read back checkpoints and loss\n",
    "def test_training_read_back():\n",
    "    pybuda.config.set_configuration_options(\n",
    "            default_df_override=pybuda.DataFormat.Float16_b,\n",
    "    )\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=PyBudaTestModuleOneOut(\"module\"))\n",
    "    tt0.place_loss_module(pybuda.op.loss.L1Loss(\"l1_loss\"))\n",
    "\n",
    "    loss_q = mp_context.Queue()\n",
    "    checkpoint_q = mp_context.Queue()\n",
    "\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "    tt0.push_to_inputs((input1, input2))\n",
    "    tt0.push_to_target_inputs(torch.rand(4, 32, 32))\n",
    "\n",
    "    pybuda.run_training(checkpoint_queue = checkpoint_q, loss_queue=loss_q)\n",
    "\n",
    "    print(\"checkpoint: \", _safe_read(checkpoint_q))\n",
    "    print(\"loss: \", _safe_read(loss_q))\n",
    "\n",
    "# Run training pipeline, with loss on CPU, read back checkpoints and loss\n",
    "#@pytest.mark.skip(reason=\"Intermittent hangs on silicon\")\n",
    "def test_training_pipeline_read_back():\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=PyBudaTestModule(\"stage0\"))\n",
    "    cpu1 = pybuda.CPUDevice(\"cpu1\", module=pybuda.PyTorchModule(\"stage1\", PyTorchTestModuleOneOut()))\n",
    "    cpu1.place_loss_module(pybuda.PyTorchModule(\"l1loss\", torch.nn.L1Loss()))\n",
    "\n",
    "    loss_q = mp_context.Queue()\n",
    "    checkpoint_q = mp_context.Queue()\n",
    "\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "    tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "    cpu1.push_to_target_inputs(torch.rand(4, 32, 32))\n",
    "\n",
    "    pybuda.run_training(checkpoint_queue = checkpoint_q, loss_queue=loss_q)\n",
    "\n",
    "    print(\"checkpoint: \", _safe_read(checkpoint_q))\n",
    "    print(\"loss: \", _safe_read(loss_q))\n",
    "\n",
    "\n",
    "#\n",
    "# Run inference pipeline on a Transformers model\n",
    "#\n",
    "def test_transformers_pipeline_inference():\n",
    "\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "\n",
    "    tokenizer = download_model(BertTokenizer.from_pretrained, \"prajjwal1/bert-tiny\")\n",
    "    input_sentence = \"BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with two objectives: Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\"\n",
    "    input_tokens = tokenizer.encode(input_sentence, max_length=128, pad_to_max_length=True)\n",
    "\n",
    "    model = download_model(BertModel.from_pretrained, \"prajjwal1/bert-tiny\", torchscript=False, add_pooling_layer=False)\n",
    "    cpu0 = pybuda.CPUDevice(\"cpu0\", module=pybuda.PyTorchModule(\"bert_embeddings\", model.embeddings))\n",
    "    tt0 = pybuda.TTDevice(\"tt1\", module=pybuda.PyTorchModule(\"bert_encoder\", model.encoder))\n",
    "\n",
    "    cpu0.push_to_inputs(torch.Tensor(input_tokens).int().unsqueeze(0))\n",
    "    output_q = pybuda.run_inference()\n",
    "\n",
    "    print(_safe_read(output_q))\n",
    "\n",
    "#\n",
    "# Run inference pipeline on a Transformers model, enabling cpu fallback on unsupported ops\n",
    "#\n",
    "def test_transformers_pipeline_fallback_inference():\n",
    "\n",
    "    from transformers import BertModel, BertTokenizer\n",
    "\n",
    "    compiler_cfg = pybuda.config._get_global_compiler_config() \n",
    "\n",
    "    tokenizer = download_model(BertTokenizer.from_pretrained, \"prajjwal1/bert-tiny\")\n",
    "    input_sentence = \"BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was pretrained with two objectives: Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\"\n",
    "    input_tokens = tokenizer.encode(input_sentence, max_length=128, pad_to_max_length=True)\n",
    "\n",
    "    model = download_model(BertModel.from_pretrained, \"prajjwal1/bert-tiny\", torchscript=False, add_pooling_layer=False)\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=pybuda.PyTorchModule(\"bert\", model))\n",
    "\n",
    "    for i in range(5):\n",
    "        tt0.push_to_inputs(torch.Tensor(input_tokens).int().unsqueeze(0))\n",
    "        output_q = pybuda.run_inference()\n",
    "        print(_safe_read(output_q))\n",
    "\n",
    "#\n",
    "# Run training through setup + manual loop of fwd/bwd/opt\n",
    "#\n",
    "def test_training_manual_loop_with_cpu_fallback():\n",
    "    from transformers import BertForMaskedLM, BertTokenizer, BertConfig \n",
    "\n",
    "    config = download_model(BertConfig.from_pretrained, \"prajjwal1/bert-tiny\")\n",
    "    model = BertForMaskedLM(config)\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=pybuda.PyTorchModule(\"bert\", model), optimizer=pybuda.optimizers.SGD(learning_rate=0.1, device_params=True))\n",
    "    tt0.place_loss_module(pybuda.PyTorchModule(\"CEL\", torch.nn.CrossEntropyLoss()))\n",
    "\n",
    "    sample_inputs = (torch.randint(config.vocab_size, (1,128)) ,)\n",
    "    sample_targets = (torch.rand(1, config.vocab_size) ,)\n",
    "\n",
    "    checkpoint_q = pybuda.initialize_pipeline(\n",
    "            training=True, \n",
    "            sample_inputs=sample_inputs,\n",
    "            sample_targets=sample_targets)\n",
    "\n",
    "\n",
    "    for step in range(2):\n",
    "        for acc_step in range(2):\n",
    "            tt0.push_to_inputs(torch.randint(config.vocab_size, (1,128)))\n",
    "            tt0.push_to_target_inputs(torch.rand(1, config.vocab_size).long())\n",
    "            pybuda.run_forward(input_count = 1)\n",
    "            pybuda.run_backward(input_count = 1, zero_grad = (acc_step == 0))\n",
    "\n",
    "        pybuda.run_optimizer(checkpoint=True)\n",
    "\n",
    "# Run training through run_training without placing on device\n",
    "# Run training by placing on device first\n",
    "# Repeated calls to run training\n",
    "# Run training in concurrent mode, then push inputs afterwards\n",
    "# Run training in concurrent mode, read checkpoints as they come out\n",
    "# Run inference on multiple devices - combinations of cpu / tt device\n",
    "\n",
    "#\n",
    "# Run training through setup + manual loop of fwd/bwd/opt\n",
    "#\n",
    "def test_training_manual_loop():\n",
    "\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=PyBudaTestModule(\"stage0\"), optimizer=pybuda.optimizers.SGD(learning_rate=0.1, device_params=True))\n",
    "    cpu1 = pybuda.CPUDevice(\"cpu1\", module=pybuda.PyTorchModule(\"stage1\", PyTorchTestModuleOneOut()),\n",
    "            optimizer_f = lambda m: torch.optim.SGD(m.parameters(), lr=0.5))\n",
    "    cpu1.place_loss_module(pybuda.PyTorchModule(\"l1loss\", torch.nn.L1Loss()))\n",
    "    \n",
    "    # Compile & initialize the pipeline for training, with given shapes\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "    checkpoint_q = pybuda.initialize_pipeline(\n",
    "            training=True, \n",
    "            sample_inputs=(input1, input2),\n",
    "            sample_targets=(torch.rand(4, 32, 32),))\n",
    "\n",
    "\n",
    "    for step in range(2):\n",
    "        for acc_step in range(2):\n",
    "            tt0.push_to_inputs((input1, input2))\n",
    "            cpu1.push_to_target_inputs(torch.rand(4, 32, 32))\n",
    "\n",
    "            pybuda.run_forward(input_count = 1)\n",
    "            pybuda.run_backward(input_count = 1, zero_grad = (acc_step == 0))\n",
    "\n",
    "        pybuda.run_optimizer(checkpoint=True)\n",
    "\n",
    "    print(\"Checkpoint: \", _safe_read(checkpoint_q))\n",
    "\n",
    "#\n",
    "# Run training through setup + manual loop of fwd/bwd, while copying final gradients\n",
    "#\n",
    "def test_training_manual_loop_no_opt():\n",
    "\n",
    "    #### Skip the test on golden. It should work, need to debug why it doesn't.\n",
    "    import os\n",
    "    if \"PYBUDA_DEVMODE\" in os.environ:\n",
    "        pytest.skip()\n",
    "    ####\n",
    "\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=PyBudaTestModule(\"stage0\"))\n",
    "    cpu1 = pybuda.CPUDevice(\"cpu1\", module=pybuda.PyTorchModule(\"stage1\", PyTorchTestModuleOneOut()))\n",
    "    cpu1.place_loss_module(pybuda.PyTorchModule(\"l1loss\", torch.nn.L1Loss()))\n",
    "    \n",
    "    # Compile & initialize the pipeline for training, with given shapes\n",
    "    pybuda.initialize_pipeline(\n",
    "            training=True, \n",
    "            sample_inputs=(torch.rand(4, 32, 32), torch.rand(4, 32, 32)), \n",
    "            sample_targets=(torch.rand(4, 32, 32),))\n",
    "\n",
    "    steps = 2\n",
    "\n",
    "    for step in range(steps):\n",
    "        for acc_step in range(1):\n",
    "    \n",
    "            input1 = torch.rand(4, 32, 32)\n",
    "            input2 = torch.rand(4, 32, 32)\n",
    "            tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "            cpu1.push_to_target_inputs(torch.rand(4, 32, 32))\n",
    "\n",
    "            pybuda.run_forward(input_count = 1)\n",
    "            pybuda.run_backward(input_count = 1, zero_grad = (acc_step == 0))\n",
    "\n",
    "        print(\"Gradients on step \", step, \": \", pybuda.get_parameter_gradients())\n",
    "\n",
    "#\n",
    "# Run training and upload new weights from host\n",
    "#\n",
    "def test_training_weight_update_on_host():\n",
    "\n",
    "    #### Skip the test on golden. It should work, need to debug why it doesn't.\n",
    "    import os\n",
    "    if \"PYBUDA_DEVMODE\" in os.environ:\n",
    "        pytest.skip()\n",
    "    ####\n",
    "\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=PyBudaTestModule(\"stage0\"))\n",
    "    cpu1 = pybuda.CPUDevice(\"cpu1\", module=pybuda.PyTorchModule(\"stage1\", PyTorchTestModuleOneOut()))\n",
    "    cpu1.place_loss_module(pybuda.PyTorchModule(\"l1loss\", torch.nn.L1Loss()))\n",
    "    \n",
    "    # Compile & initialize the pipeline for training, with given shapes\n",
    "    pybuda.initialize_pipeline(training=True, \n",
    "            sample_inputs=(torch.rand(4, 32, 32), torch.rand(4, 32, 32)), \n",
    "            sample_targets=(torch.rand(4, 32, 32),))\n",
    "\n",
    "    for _ in range(2):\n",
    "        input1 = torch.rand(4, 32, 32)\n",
    "        input2 = torch.rand(4, 32, 32)\n",
    "        tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "        cpu1.push_to_target_inputs(torch.rand(4, 32, 32))\n",
    "\n",
    "    # Run fwd/bwd to calculate parameter gradients\n",
    "    pybuda.run_forward(input_count = 1)\n",
    "    pybuda.run_backward(input_count = 1, zero_grad = True)\n",
    "\n",
    "    # Retrieve weights and gradients, and use host optimizer to update weights\n",
    "    grads = pybuda.get_parameter_gradients(tt0)\n",
    "    params = pybuda.get_parameter_checkpoint(tt0)\n",
    "    for name in params[0]:\n",
    "        params[0][name].value().grad = grads[0][name].value()\n",
    "    opt = torch.optim.SGD([p.value() for p in params[0].values()], lr=10.0)\n",
    "    opt.step()\n",
    "\n",
    "    # Push new weights to the device\n",
    "    pybuda.update_device_parameters(tt0, params)\n",
    "\n",
    "    # Run again with new weights\n",
    "    pybuda.run_forward(input_count = 1)\n",
    "    pybuda.run_backward(input_count = 1, zero_grad = True)\n",
    "\n",
    "# \n",
    "# Run inference pipeline and provide mp queues for device-to-device data\n",
    "#\n",
    "def test_inference_device_to_device_data():\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=PyBudaTestModule(\"stage0\"))\n",
    "    cpu1 = pybuda.CPUDevice(\"cpu1\", module=pybuda.PyTorchModule(\"stage1\", PyTorchTestModule()))\n",
    "    cpu2 = pybuda.CPUDevice(\"cpu2\", module=pybuda.PyTorchModule(\"stage2\", PyTorchTestModuleOneOut()))\n",
    "    \n",
    "    # Compile & initialize the pipeline for inference, and provide d2d mp queues to store device-to-device data in for further analysis\n",
    "    tt0_output_q = mp_context.Queue()\n",
    "    cpu1_output_q = mp_context.Queue()\n",
    "    pybuda.initialize_pipeline(training=False, d2d_fwd_queues=[tt0_output_q, cpu1_output_q], \n",
    "            sample_inputs=(torch.rand(4, 32, 32), torch.rand(4, 32, 32) ))\n",
    "\n",
    "    for _ in range(2):\n",
    "        input1 = torch.rand(4, 32, 32)\n",
    "        input2 = torch.rand(4, 32, 32)\n",
    "        tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "    # Run fwd\n",
    "    pybuda.run_forward(input_count = 1)\n",
    "\n",
    "    # Read d2d queues\n",
    "    print(_safe_read(tt0_output_q))\n",
    "    print(_safe_read(cpu1_output_q))\n",
    "\n",
    "# \n",
    "# Run training pipeline and provide mp queues for device-to-device data\n",
    "#\n",
    "\n",
    "def test_training_device_to_device_data():\n",
    "    \n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=PyBudaTestModule(\"stage0\"))\n",
    "    cpu1 = pybuda.CPUDevice(\"cpu1\", module=pybuda.PyTorchModule(\"stage1\", PyTorchTestModule()))\n",
    "    cpu2 = pybuda.CPUDevice(\"cpu2\", module=pybuda.PyTorchModule(\"stage2\", PyTorchTestModuleOneOut()))\n",
    "    cpu2.place_loss_module(pybuda.PyTorchModule(\"l1loss\", torch.nn.L1Loss()))\n",
    "    \n",
    "    # Compile & initialize the pipeline for inference, and provide d2d mp queues to store device-to-device data in for further analysis\n",
    "    tt0_output_q = mp_context.Queue()\n",
    "    cpu1_output_q = mp_context.Queue()\n",
    "    cpu1_bwd_output_q = mp_context.Queue()\n",
    "    cpu2_bwd_output_q = mp_context.Queue()\n",
    "    pybuda.initialize_pipeline(\n",
    "            training=True, \n",
    "            d2d_fwd_queues=[tt0_output_q, cpu1_output_q], \n",
    "            d2d_bwd_queues=[cpu1_bwd_output_q, cpu2_bwd_output_q], \n",
    "            sample_inputs=(torch.rand(4, 32, 32), torch.rand(4, 32, 32)), \n",
    "            sample_targets=(torch.rand(4, 32, 32),))\n",
    "\n",
    "    for _ in range(2):\n",
    "        input1 = torch.rand(4, 32, 32)\n",
    "        input2 = torch.rand(4, 32, 32)\n",
    "        tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "        cpu2.push_to_target_inputs(torch.rand(4, 32, 32))\n",
    "\n",
    "    # Run fwd/bwd \n",
    "    pybuda.run_forward()\n",
    "    pybuda.run_backward(zero_grad = True)\n",
    "\n",
    "    # Read d2d queues\n",
    "    print(_safe_read(tt0_output_q))\n",
    "    print(_safe_read(cpu1_output_q))\n",
    "    print(_safe_read(cpu1_bwd_output_q))\n",
    "    print(_safe_read(cpu2_bwd_output_q))\n",
    "    pybuda.get_parameter_gradients(tt0)\n",
    "\n",
    "#\n",
    "# Override data formats\n",
    "#\n",
    "def test_data_formats_input_override():\n",
    "\n",
    "    mod = PyBudaTestModule(\"mod\")\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=mod)\n",
    "\n",
    "    # Explicitly set data formats for parameters and inputs\n",
    "    mod.weights1.set_data_format(pybuda.DataFormat.Float16)\n",
    "    mod.weights2.set_data_format(pybuda.DataFormat.Float16)\n",
    "    input1 = torch.rand(4, 32, 32, dtype=torch.float16)\n",
    "    input2 = torch.rand(4, 32, 32, dtype=torch.float16)\n",
    "    tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "    pybuda.run_inference()\n",
    "\n",
    "def test_data_formats_fp32_fallback():\n",
    "    \n",
    "    # On this device, fall back to Float16 wherever Float32 is used\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=PyBudaTestModule(\"mod\"), fp32_fallback=pybuda.DataFormat.Float16)\n",
    "\n",
    "    # Push Float32, which will be converted to Float16 due to fp32_fallback\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "    tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "    pybuda.run_inference()\n",
    "\n",
    "def test_data_formats_op_override():\n",
    "    \n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=PyBudaTestModule(\"mod\"))\n",
    "\n",
    "    # Use API to set manual data format override on an op\n",
    "    pybuda.configure_mixed_precision(name_regex=\"matmul1\", output_df=pybuda.DataFormat.Bfp8_b)\n",
    "    \n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "    tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "    pybuda.run_inference()\n",
    "\n",
    "class TorchSchedulerWithWarmupAndDecay(pybuda.torch_schedulers.TorchLearningRateScheduler):\n",
    "    def __init__(self, optimizer):\n",
    "        super().__init__(optimizer)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return [self.optimizer.param_groups[0][\"lr\"] + 1]\n",
    "    \n",
    "    def step(self):\n",
    "        super().step()\n",
    "        print(f\"Torch optimizer learning rate updated to {self.optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "\n",
    "class TestScheduler(LearningRateScheduler):\n",
    "        def __init__(self, optimizer):\n",
    "            super().__init__(optimizer)\n",
    "        \n",
    "        def get_lr(self):\n",
    "            return self.optimizer.learning_rate + 1\n",
    "        \n",
    "        def step(self):\n",
    "            super().step()\n",
    "            print(f\"Pybuda optimizer learning rate updated to {self.optimizer.learning_rate}\")\n",
    "        \n",
    "        def get_pytorch_scheduler(self, optimizer: torch.optim.Optimizer):\n",
    "            if self.torch_scheduler is None:\n",
    "                self.torch_scheduler = TorchSchedulerWithWarmupAndDecay(\n",
    "                    optimizer=optimizer\n",
    "                )\n",
    "            \n",
    "            return self.torch_scheduler\n",
    "\n",
    "\n",
    "# Run the learning rate scheduler across 100 steps to\n",
    "# show how optimizer learning rate gets updated\n",
    "def test_learning_rate_scheduler():\n",
    "            \n",
    "    lr = 1\n",
    "    optimizer = pybuda.optimizers.SGD(learning_rate=lr, device_params=True)\n",
    "    scheduler = TestScheduler(optimizer=optimizer)\n",
    "    \n",
    "    tt0 = pybuda.TTDevice(\n",
    "        \"tt0\", \n",
    "        module=PyBudaTestModuleOneOut(\"stage0\"), \n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "    cpu1 = pybuda.CPUDevice(\n",
    "        \"cpu1\",\n",
    "        module=pybuda.PyTorchModule(\n",
    "            \"stage1\",\n",
    "            PyTorchTestModuleOneInputAndOneOut()\n",
    "        ),\n",
    "        optimizer_f=lambda module: torch.optim.SGD(module.parameters(), lr=lr),\n",
    "        scheduler_f=lambda optimizer: scheduler.get_pytorch_scheduler(optimizer)\n",
    "    )\n",
    "    cpu1.place_loss_module(\n",
    "        pybuda.PyTorchModule(\n",
    "            \"loss\",\n",
    "            PyTorchLoss()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    sequential = True\n",
    "    pybuda.initialize_pipeline(training=True, \n",
    "            sample_inputs=(torch.rand(4, 32, 32), torch.rand(4, 32, 32)), \n",
    "            sample_targets=(torch.rand(4, 32, 32),), _sequential=sequential)\n",
    "\n",
    "    for _ in range(100):\n",
    "        pybuda.run_schedulers(sequential)\n",
    "    \n",
    "    \n",
    "    \n",
    "def test_specific_chip_id():\n",
    "    \"\"\"\n",
    "    Run inference on a specific chip on a multi-chip system\n",
    "    \"\"\"\n",
    "    num_devices = len(pybuda.detect_available_devices())\n",
    "\n",
    "    if num_devices < 2:\n",
    "        pytest.skip(\"Need at least 2 devices to run chip-id test\")\n",
    "\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "\n",
    "    # Create a TT device, on last available chip\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", chip_ids=[num_devices-1])\n",
    "\n",
    "    # Place a module on the device\n",
    "    tt0.place_module(PyBudaTestModule(\"last_chip\"))\n",
    "\n",
    "    # Push intputs to the device\n",
    "    tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "    # Run pipeline, and read the outputs\n",
    "    output_q = pybuda.run_inference()\n",
    "    output = _safe_read(output_q)\n",
    "    print(output)\n",
    "\n",
    "def _run_on_chip(chip_id: int):\n",
    "\n",
    "    # Each process needs to have its own temporary dir\n",
    "    pybuda.set_configuration_options(backend_output_dir=f\"tt_build/test_out_chip_{chip_id}\")\n",
    "\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "\n",
    "    # Create a TT device, on last available chip\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", chip_ids=[chip_id])\n",
    "\n",
    "    # Place a module on the device\n",
    "    tt0.place_module(PyBudaTestModule(f\"chip_{chip_id}\"))\n",
    "\n",
    "    # Push intputs to the device\n",
    "    tt0.push_to_inputs((input1, input2))\n",
    "\n",
    "    # Run pipeline, and read the outputs\n",
    "    output_q = pybuda.run_inference()\n",
    "    output = _safe_read(output_q)\n",
    "    print(\"From chip \", chip_id, \":\", output)\n",
    "\n",
    "    # Clean up the process so we can end it cleanly\n",
    "    pybuda.shutdown()\n",
    "\n",
    "\n",
    "def test_parallel_chips():\n",
    "    \"\"\"\n",
    "    Run different models on multiple chips at the same time\n",
    "    \"\"\"\n",
    "    pytest.skip(\"Appears to hang now\")\n",
    "    num_devices = len(pybuda.detect_available_devices())\n",
    "\n",
    "    if num_devices < 2:\n",
    "        pytest.skip(\"Need at least 2 devices to run parallel chip test\")\n",
    "\n",
    "    procs = []\n",
    "    for i in range(num_devices):\n",
    "        p = mp_context.Process(target=_run_on_chip, args=(i,))\n",
    "        p.start()\n",
    "        procs.append(p)\n",
    "\n",
    "    for i, p in enumerate(procs):\n",
    "        p.join()\n",
    "\n",
    "def test_tti_inference_save_and_load():\n",
    "    available_devices = pybuda.detect_available_devices()\n",
    "    if available_devices and available_devices[0] == BackendDevice.Grayskull:\n",
    "        tt0 = pybuda.TTDevice(\n",
    "            \"tt0\",\n",
    "            arch=BackendDevice.Grayskull,\n",
    "            devtype=BackendType.Golden,\n",
    "        )\n",
    "    else:\n",
    "        tt0 = pybuda.TTDevice(\n",
    "            \"tt0\",\n",
    "            arch=BackendDevice.Wormhole_B0,\n",
    "            devtype=BackendType.Golden,\n",
    "        )\n",
    "\n",
    "\n",
    "    module = PyBudaTestModule(\"test_pybuda_module\")\n",
    "    tt0.place_module(module)\n",
    "\n",
    "    # Saving to Archive\n",
    "    input_shape = (1, 1, 32, 32)\n",
    "    input1, input2  = torch.rand(*input_shape), torch.rand(*input_shape)\n",
    "    device_img = tt0.compile_to_image(\n",
    "        img_path=\"device_images/test_tt0.tti\", \n",
    "        training=False,\n",
    "        sample_inputs=(input1, input2),\n",
    "    )\n",
    "    pybuda_reset()  # flush the global state that lingers around for test\n",
    "\n",
    "    # Loading from Archive\n",
    "    tt1 = pybuda.TTDevice.load_image(img_path=\"device_images/test_tt0.tti\")\n",
    "    tt1.push_to_inputs((input1, input2))\n",
    "    output_q = pybuda.run_inference()\n",
    "    output = _safe_read(output_q)\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"hoist_tms\", [True, False])\n",
    "def test_nop_insertion_api(hoist_tms):\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=PyBudaTestQueryKeyModule(f\"query_key_module_hoist_tms_{hoist_tms}\"))\n",
    "\n",
    "    # Use API to set manual data format override on an op\n",
    "    pybuda.insert_nop(\"mha_key\", \"mha_as\", hoist_tms=hoist_tms)\n",
    "    microbatch_size, seq_len, hidden_dim = (1, 128, 128)\n",
    "    encoder_input = torch.rand(microbatch_size, seq_len, hidden_dim)\n",
    "\n",
    "    tt0.push_to_inputs((encoder_input))\n",
    "    pybuda.run_inference()\n",
    "\n",
    "@pytest.mark.parametrize(\"hoist_tms\", [True, False])\n",
    "def test_nop_fork_insertion_api(hoist_tms):\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=PyBudaTestQueryKeyModule(f\"forking_nop_insertion{hoist_tms}\"))\n",
    "\n",
    "    # Use API to set manual data format override on an op\n",
    "    pybuda.insert_nop(\"encoder_input\", [\"mha_key\", \"mha_query\"], hoist_tms=hoist_tms)\n",
    "    microbatch_size, seq_len, hidden_dim = (1, 128, 128)\n",
    "    encoder_input = torch.rand(microbatch_size, seq_len, hidden_dim)\n",
    "\n",
    "    tt0.push_to_inputs((encoder_input))\n",
    "    pybuda.run_inference()\n",
    "\n",
    "@pytest.mark.parametrize(\"hoist_tms\", [True, False])\n",
    "def test_nop_daily_chain_insertion_api(hoist_tms):\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=PyBudaTestForkWithThreeUsers(f\"daisy_chain_nop_insertion{hoist_tms}\"))\n",
    "\n",
    "    # Use API to set manual data format override on an op\n",
    "    pybuda.insert_nop(\"encoder_input\", [\"mm_a\", \"mm_b\", \"mm_c\"], hoist_tms=hoist_tms)\n",
    "    pybuda.insert_nop(\"buffer_0_encoder_input_mm_a\", [\"mm_b\", \"mm_c\"], hoist_tms=hoist_tms)\n",
    "    pybuda.insert_nop(\"buffer_0_buffer_0_encoder_input_mm_a_mm_b\", [\"mm_c\"], hoist_tms=hoist_tms)\n",
    "    microbatch_size, seq_len, hidden_dim = (1, 128, 128)\n",
    "    encoder_input = torch.rand(microbatch_size, seq_len, hidden_dim)\n",
    "\n",
    "    tt0.push_to_inputs((encoder_input))\n",
    "    pybuda.run_inference()\n",
    "\n",
    "def test_dram_channel_override():\n",
    "    tt0 = pybuda.TTDevice(\"tt0\", module=PyBudaTestModule(f\"dram_channel_override\"))\n",
    "\n",
    "    # Use API to set manual data format override on an op\n",
    "    input1 = torch.rand(4, 32, 32)\n",
    "    input2 = torch.rand(4, 32, 32)\n",
    "    pybuda.config.override_dram_queue_placement(\"e2e_matmul1_0\", channel=0)\n",
    "    pybuda.config.set_epoch_break(\"matmul2\")\n",
    "\n",
    "    tt0.push_to_inputs((input1, input2))\n",
    "    pybuda.run_inference()\n",
    "\n",
    "@pytest.mark.parametrize(\"loss\", [\"l1\", \"mse\"])\n",
    "def test_loss_module_on_ttdevice(loss):\n",
    "    import torch.nn as nn\n",
    "    class Lin(nn.Module):\n",
    "        def __init__(self, d_model):\n",
    "            super(Lin, self).__init__()\n",
    "            self.input_linear = nn.Linear(1, d_model)\n",
    "\n",
    "        def forward(self, src):\n",
    "            output = self.input_linear(src)\n",
    "            return output\n",
    "\n",
    "    model = Lin(1)\n",
    "    tt0 = pybuda.TTDevice(\n",
    "        \"tt0\",\n",
    "        module=pybuda.PyTorchModule(\"lin\", model),\n",
    "        optimizer=pybuda.optimizers.SGD(learning_rate=0.1, device_params=True)\n",
    "    )\n",
    "    if loss == \"mse\":\n",
    "        tt0.place_loss_module(pybuda.PyTorchModule(\"mse_loss\", nn.MSELoss()))\n",
    "    else:\n",
    "        tt0.place_loss_module(pybuda.PyTorchModule(\"l1_loss\", nn.L1Loss()))\n",
    "\n",
    "    inputs = torch.rand(1, 1)\n",
    "    targets = torch.rand(1, 1)\n",
    "\n",
    "    # Initialize pipeline\n",
    "    checkpoint_q = pybuda.initialize_pipeline(\n",
    "       training=True,\n",
    "       sample_inputs=(inputs,),\n",
    "       sample_targets=(targets,)\n",
    "    )\n",
    "\n",
    "    tt0.push_to_inputs(inputs)\n",
    "    tt0.push_to_target_inputs(targets)\n",
    "    pybuda.run_forward(input_count=1)\n",
    "    pybuda.run_backward(input_count=1, zero_grad=True)\n",
    "    pybuda.run_optimizer(checkpoint=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
